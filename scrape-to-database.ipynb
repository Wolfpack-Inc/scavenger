{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:35:02.206632Z",
     "start_time": "2019-10-11T17:35:02.203006Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import xml.etree.ElementTree as ET\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imagehash\n",
    "\n",
    "SCRAPE_DIR = '.scrape_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to fetch the xml files\n",
    "1. Install `pip install oaiharvest`\n",
    "2. Create a folder in this repo called '.scrape_output'\n",
    "3. `cd .scrape_output`\n",
    "4. Run the following command: `oai-harvest --set=Beelddocument --metadataPrefix=ese https://denbosch.hosting.deventit.net/atlantispubliek/oai.axd` (This may take a while)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:35:02.719875Z",
     "start_time": "2019-10-11T17:35:02.716419Z"
    }
   },
   "source": [
    "### Test code to get the format of the xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:36:33.477067Z",
     "start_time": "2019-10-11T17:36:33.465439Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('.scrape_output/14433440.ese.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "items = []\n",
    "\n",
    "for child in root:\n",
    "    prefix, has_namespace, postfix = child.tag.partition('}')\n",
    "    items.append({\n",
    "        'key': postfix,\n",
    "        'value': child.text\n",
    "    })\n",
    "\n",
    "pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of the XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:38:10.587619Z",
     "start_time": "2019-10-11T17:38:10.370397Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_xml_files = [SCRAPE_DIR + '/' + name for name in os.listdir(SCRAPE_DIR)]\n",
    "image_xml_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through the images and transform them into a dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:40:23.813746Z",
     "start_time": "2019-10-11T17:40:23.806436Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "images = []\n",
    "\n",
    "# xml namespace\n",
    "ns = {\n",
    "    'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "    'europeana': 'http://www.europeana.eu/schemas/ese/',\n",
    "    'dcterms': 'http://purl.org/dc/terms/'\n",
    "}\n",
    "\n",
    "# No image hash\n",
    "no_img = '00ff6767777f8300'\n",
    "\n",
    "for idx, image_xml in tqdm(enumerate(image_xml_files)):\n",
    "    \n",
    "    # Parse as xml\n",
    "    tree = ET.parse(image_xml)\n",
    "    \n",
    "    # Get the root of the tree\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Dictionary to store the image attributes\n",
    "    image_attribs = {}\n",
    "    \n",
    "    # Location\n",
    "    coverage = root.find('dc:coverage', ns)\n",
    "    \n",
    "    if coverage == None:\n",
    "        continue\n",
    "    else:\n",
    "        coverage = coverage.text.split(';')\n",
    "        if coverage[0] != \"'s-Hertogenbosch\":\n",
    "            continue\n",
    "        elif coverage[1] == '':\n",
    "            continue\n",
    "        \n",
    "    street = coverage[1]\n",
    "    image_attribs['street'] = street\n",
    "        \n",
    "    # Medium\n",
    "    medium = root.find('dcterms:medium', ns)\n",
    "    \n",
    "    # Make sure the image is a photo\n",
    "    if medium == None:\n",
    "        continue\n",
    "    elif medium.text != 'Foto':\n",
    "        continue\n",
    "    \n",
    "    # Image url\n",
    "    image_url = root.find('europeana:object', ns).text\n",
    "    \n",
    "    # Check if the image exists\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        hasj = str(imagehash.average_hash(image))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # If there is no image\n",
    "    if hasj == no_img:\n",
    "        image_attribs['available'] = 0\n",
    "    else: image_attribs['available'] = 1\n",
    "\n",
    "    # Size of the image\n",
    "    image_attribs['width'], image_attribs['height'] = image.size\n",
    "    \n",
    "    # Image url\n",
    "    image_url = root.find('europeana:object', ns).text\n",
    "    \n",
    "    # Image url\n",
    "    image_attribs['url'] = image_url\n",
    "    \n",
    "    # Get the title\n",
    "    image_attribs['title'] = root.find('dc:title', ns).text.strip()\n",
    "            \n",
    "    # Date\n",
    "    if root.find('dc:date', ns).text == None or len(root.find('dc:date', ns).text[-4:]) < 4 or '/' in root.find('dc:date', ns).text[-4:]:\n",
    "        image_attribs['year'] = None\n",
    "    else: \n",
    "        image_attribs['year'] = root.find('dc:date', ns).text[-4:]\n",
    "        \n",
    "    \n",
    "    # Add to list\n",
    "    images.append(image_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:40:24.537459Z",
     "start_time": "2019-10-11T17:40:24.528787Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(images)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe into MySQL\n",
    "import sqlalchemy\n",
    "database_username = 'remote'\n",
    "database_password = 'EtrPCEc0jt'\n",
    "database_ip       = '165.22.199.122'\n",
    "database_name     = 'scavenger'\n",
    "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "df.to_sql(con=database_connection, name='images', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# HERE app_id & app_code\n",
    "APP_ID = 'oidKwC98hLcIQBaboIwS'\n",
    "APP_CODE = 'G6gKSvk45y4fF14hwpIeYg'\n",
    "\n",
    "# Function that collects multiple location-coordinates of a single street\n",
    "def get_location(streetname):\n",
    "    '''Returns list of latitude and longitude coordinates for a given street'''\n",
    "    \n",
    "    if streetname == '':\n",
    "        return None\n",
    "    \n",
    "    url = 'https://geocoder.api.here.com/6.2/geocode.json?app_id=' + APP_ID + '&app_code=' + APP_CODE + '&searchtext='\n",
    "\n",
    "    r = requests.get(url + streetname + \", 's-Hertogenbosch\").json()\n",
    "    \n",
    "    if r['Response']['View'] != []:\n",
    "        \n",
    "        loc_data = r['Response']['View'][0]['Result'][0]['Location']\n",
    "        \n",
    "        #DisplayPosition\n",
    "        loc1 = loc_data['DisplayPosition']\n",
    "    \n",
    "        #NavigationPosition\n",
    "        loc2 = loc_data['NavigationPosition'][0]\n",
    "    \n",
    "        #MapView\n",
    "        loc3A = loc_data['MapView']['TopLeft']\n",
    "        \n",
    "        loc3B = loc_data['MapView']['BottomRight']\n",
    "        \n",
    "        #Avg coordinates b/w Mapview TopLeft & Mapview BottomRight\n",
    "        loc4 = ((loc3A['Latitude'] + loc3B['Latitude']) / 2, (loc3A['Longitude'] + loc3B['Longitude']) / 2 )\n",
    "        \n",
    "        return [loc1['Latitude'], loc1['Longitude'], \n",
    "                loc2['Latitude'], loc2['Longitude'], \n",
    "                loc3A['Latitude'], loc3A['Longitude'],\n",
    "                loc3B['Latitude'], loc3B['Longitude'], \n",
    "                loc4[0], loc4[1]]\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All streetnames present in the dataframe\n",
    "street_names = list(df['street'].unique())\n",
    "\n",
    "# Remove from street_names list: nan value at index location 66, '' at index location 1, and 'onbekend' at index location 537\n",
    "del street_names[66]\n",
    "del street_names[1]\n",
    "del street_names[537]\n",
    "\n",
    "# Generate dictionary with street location and all corresponding locations\n",
    "street_locations = {street : get_location(street) for street in street_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all streets that API did NOT find location coordinates for\n",
    "failed_location = [street[0] for street in street_locations.items() if street[1] == None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove streets from street_locations dictionary that do not have any coordinates\n",
    "[street_locations.pop(street) for street in failed_location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column order of coordinates (dis = display location, nav = navigation location, map_top = mapview topleft, map_bot = mapview bottomright, map_avg = mapview average)\n",
    "column_order = ['dis_lat', 'dis_long', 'nav_lat', 'nav_long', 'map_top_lat', 'map_top_long', 'map_bot_lat', 'map_bot_long', 'map_avg_lat', 'map_avg_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with streets as rows and coordinates as columns\n",
    "loc_df = pd.DataFrame.from_dict(street_locations, orient='index', columns=column_order)\n",
    "loc_df.reset_index(level=0, inplace=True)\n",
    "loc_df.rename(columns={'index':'street'}, inplace=True)\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe into MySQL\n",
    "import sqlalchemy\n",
    "database_username = 'remote'\n",
    "database_password = 'EtrPCEc0jt'\n",
    "database_ip       = '165.22.199.122'\n",
    "database_name     = 'scavenger'\n",
    "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "loc_df.to_sql(con=database_connection, name='locations', if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
